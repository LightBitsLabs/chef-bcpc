diff -ru nova/compute/manager.py nova.new/compute/manager.py
--- nova/compute/manager.py	2014-04-03 14:49:46.000000000 -0400
+++ nova.new/compute/manager.py	2014-04-19 13:44:10.398837410 -0400
@@ -668,9 +668,10 @@
                 return
 
         net_info = compute_utils.get_nw_info_for_instance(instance)
-
-        self.driver.plug_vifs(instance, net_info)
-
+        try:
+            self.driver.plug_vifs(instance, net_info)
+        except NotImplementedError as e:
+            LOG.debug(e, instance=instance)
         if instance['task_state'] == task_states.RESIZE_MIGRATING:
             # We crashed during resize/migration, so roll back for safety
             try:
@@ -4155,8 +4156,10 @@
             # but we must do it explicitly here when block_migration
             # is false, as the network devices at the source must be
             # torn down
-            self.driver.unplug_vifs(instance_ref, network_info)
-
+            try:
+                self.driver.unplug_vifs(instance_ref, network_info)
+            except NotImplementedError as e:
+                LOG.debug(e, instance=instance_ref)
         # NOTE(tr3buchet): tear down networks on source host
         self.network_api.setup_networks_on_host(ctxt, instance_ref,
                                                 self.host, teardown=True)
Only in nova.new/compute: manager.py.orig
diff -ru nova/openstack/common/rpc/impl_qpid.py nova.new/openstack/common/rpc/impl_qpid.py
--- nova/openstack/common/rpc/impl_qpid.py	2014-04-03 14:49:46.000000000 -0400
+++ nova.new/openstack/common/rpc/impl_qpid.py	2014-04-19 13:44:34.822914245 -0400
@@ -502,7 +502,7 @@
             if self.connection.opened():
                 try:
                     self.connection.close()
-                except qpid_exceptions.ConnectionError:
+                except qpid_exceptions.MessagingError:
                     pass
 
             broker = self.brokers[attempt % len(self.brokers)]
@@ -539,7 +539,7 @@
             try:
                 return method(*args, **kwargs)
             except (qpid_exceptions.Empty,
-                    qpid_exceptions.ConnectionError) as e:
+                    qpid_exceptions.MessagingError) as e:
                 if error_callback:
                     error_callback(e)
                 self.reconnect()
Only in nova.new/openstack/common/rpc: impl_qpid.py.orig
diff -ru nova/virt/docker/driver.py nova.new/virt/docker/driver.py
--- nova/virt/docker/driver.py	2014-04-03 14:49:46.000000000 -0400
+++ nova.new/virt/docker/driver.py	2014-04-19 13:44:36.794919437 -0400
@@ -99,11 +99,13 @@
 
     def plug_vifs(self, instance, network_info):
         """Plug VIFs into networks."""
-        pass
+        msg = _("VIF plugging is not supported by the Docker driver.")
+        raise NotImplementedError(msg)
 
     def unplug_vifs(self, instance, network_info):
         """Unplug VIFs from networks."""
-        pass
+        msg = _("VIF unplugging is not supported by the Docker driver.")
+        raise NotImplementedError(msg)
 
     def find_container_by_name(self, name):
         for info in self.list_instances(inspect=True):
diff -ru nova/virt/hyperv/driver.py nova.new/virt/hyperv/driver.py
--- nova/virt/hyperv/driver.py	2014-04-03 14:49:46.000000000 -0400
+++ nova.new/virt/hyperv/driver.py	2014-04-19 13:44:36.794919437 -0400
@@ -151,10 +151,14 @@
             ctxt, instance_ref, dest_check_data)
 
     def plug_vifs(self, instance, network_info):
-        LOG.debug(_("plug_vifs called"), instance=instance)
+        """Plug VIFs into networks."""
+        msg = _("VIF plugging is not supported by the Hyper-V driver.")
+        raise NotImplementedError(msg)
 
     def unplug_vifs(self, instance, network_info):
-        LOG.debug(_("unplug_vifs called"), instance=instance)
+        """Unplug VIFs from networks."""
+        msg = _("VIF unplugging is not supported by the Hyper-V driver.")
+        raise NotImplementedError(msg)
 
     def ensure_filtering_rules_for_instance(self, instance_ref, network_info):
         LOG.debug(_("ensure_filtering_rules_for_instance called"),
diff -ru nova/virt/powervm/driver.py nova.new/virt/powervm/driver.py
--- nova/virt/powervm/driver.py	2014-04-03 14:49:46.000000000 -0400
+++ nova.new/virt/powervm/driver.py	2014-04-19 13:44:42.510933664 -0400
@@ -92,9 +92,13 @@
 
     def plug_vifs(self, instance, network_info):
         """Plug VIFs into networks."""
-        LOG.debug(_('Network injection is not supported by the '
-                    'PowerVM driver.'), instance)
-        pass
+        msg = _("VIF plugging is not supported by the PowerVM driver.")
+        raise NotImplementedError(msg)
+
+    def unplug_vifs(self, instance, network_info):
+        """Unplug VIFs from networks."""
+        msg = _("VIF unplugging is not supported by the PowerVM driver.")
+        raise NotImplementedError(msg)
 
     def macs_for_instance(self, instance):
         return self._powervm.macs_for_instance(instance)
diff -ru nova/virt/vmwareapi/driver.py nova.new/virt/vmwareapi/driver.py
--- nova/virt/vmwareapi/driver.py	2014-04-03 14:49:46.000000000 -0400
+++ nova.new/virt/vmwareapi/driver.py	2014-04-19 13:44:42.510933664 -0400
@@ -372,14 +372,6 @@
         """inject network info for specified instance."""
         self._vmops.inject_network_info(instance, network_info)
 
-    def plug_vifs(self, instance, network_info):
-        """Plug VIFs into networks."""
-        self._vmops.plug_vifs(instance, network_info)
-
-    def unplug_vifs(self, instance, network_info):
-        """Unplug VIFs from networks."""
-        self._vmops.unplug_vifs(instance, network_info)
-
     def list_instance_uuids(self):
         """List VM instance UUIDs."""
         uuids = self._vmops.list_instances()
@@ -733,16 +725,6 @@
         _vmops = self._get_vmops_for_compute_node(instance['node'])
         _vmops.inject_network_info(instance, network_info)
 
-    def plug_vifs(self, instance, network_info):
-        """Plug VIFs into networks."""
-        _vmops = self._get_vmops_for_compute_node(instance['node'])
-        _vmops.plug_vifs(instance, network_info)
-
-    def unplug_vifs(self, instance, network_info):
-        """Unplug VIFs from networks."""
-        _vmops = self._get_vmops_for_compute_node(instance['node'])
-        _vmops.unplug_vifs(instance, network_info)
-
 
 class VMwareAPISession(object):
     """
diff -ru nova/virt/vmwareapi/vmops.py nova.new/virt/vmwareapi/vmops.py
--- nova/virt/vmwareapi/vmops.py	2014-04-03 14:49:46.000000000 -0400
+++ nova.new/virt/vmwareapi/vmops.py	2014-04-19 13:44:42.510933664 -0400
@@ -171,7 +171,8 @@
         LOG.debug(_("Deleted the datastore file"), instance=instance)
 
     def spawn(self, context, instance, image_meta, injected_files,
-              admin_password, network_info, block_device_info=None):
+              admin_password, network_info, block_device_info=None,
+              instance_name=None):
         """
         Creates a VM instance.
 
@@ -282,9 +283,13 @@
 
         vif_infos = _get_vif_infos()
 
+        # Get the instance name. In some cases this may differ from the 'uuid',
+        # for example when the spawn of a rescue instance takes place.
+        if not instance_name:
+            instance_name = instance['uuid']
         # Get the create vm config spec
         config_spec = vm_util.get_vm_create_spec(
-                            client_factory, instance,
+                            client_factory, instance, instance_name,
                             data_store_name, vif_infos, os_type)
 
         def _execute_create_vm():
@@ -300,7 +305,7 @@
             LOG.debug(_("Created VM on the ESX host"), instance=instance)
 
         _execute_create_vm()
-        vm_ref = vm_util.get_vm_ref(self._session, instance)
+        vm_ref = vm_util.get_vm_ref_from_name(self._session, instance_name)
 
         # Set the machine.id parameter of the instance to inject
         # the NIC configuration inside the VM
@@ -844,8 +849,6 @@
     def reboot(self, instance, network_info):
         """Reboot a VM instance."""
         vm_ref = vm_util.get_vm_ref(self._session, instance)
-        self.plug_vifs(instance, network_info)
-
         lst_properties = ["summary.guest.toolsStatus", "runtime.powerState",
                           "summary.guest.toolsRunningStatus"]
         props = self._session._call_method(vim_util, "get_object_properties",
@@ -898,21 +901,23 @@
             except Exception as excep:
                 LOG.warn(_("In vmwareapi:vmops:delete, got this exception"
                            " while destroying the VM: %s") % str(excep))
-
-            if network_info:
-                self.unplug_vifs(instance, network_info)
         except Exception as exc:
             LOG.exception(exc, instance=instance)
 
-    def destroy(self, instance, network_info, destroy_disks=True):
+    def destroy(self, instance, network_info, destroy_disks=True,
+                instance_name=None):
         """
         Destroy a VM instance. Steps followed are:
         1. Power off the VM, if it is in poweredOn state.
         2. Un-register a VM.
         3. Delete the contents of the folder holding the VM related data.
         """
+        # Get the instance name. In some cases this may differ from the 'uuid',
+        # for example when the spawn of a rescue instance takes place.
+        if not instance_name:
+            instance_name = instance['uuid']
         try:
-            vm_ref = vm_util.get_vm_ref(self._session, instance)
+            vm_ref = vm_util.get_vm_ref_from_name(self._session, instance_name)
             lst_properties = ["config.files.vmPathName", "runtime.powerState",
                               "datastore"]
             props = self._session._call_method(vim_util,
@@ -945,10 +950,6 @@
             except Exception as excep:
                 LOG.warn(_("In vmwareapi:vmops:destroy, got this exception"
                            " while un-registering the VM: %s") % str(excep))
-
-            if network_info:
-                self.unplug_vifs(instance, network_info)
-
             # Delete the folder holding the VM related content on
             # the datastore.
             if destroy_disks:
@@ -1041,9 +1042,10 @@
         self.power_off(instance)
         r_instance = copy.deepcopy(instance)
         r_instance['name'] = r_instance['name'] + self._rescue_suffix
-        r_instance['uuid'] = r_instance['uuid'] + self._rescue_suffix
+        instance_name = r_instance['uuid'] + self._rescue_suffix
         self.spawn(context, r_instance, image_meta,
-                   None, None, network_info)
+                   None, None, network_info,
+                   instance_name=instance_name)
 
         # Attach vmdk to the rescue VM
         hardware_devices = self._session._call_method(vim_util,
@@ -1055,11 +1057,8 @@
 
         # Figure out the correct unit number
         unit_number = unit_number + 1
-        rescue_vm_ref = vm_util.get_vm_ref_from_uuid(self._session,
-                                                     r_instance['uuid'])
-        if rescue_vm_ref is None:
-            rescue_vm_ref = vm_util.get_vm_ref_from_name(self._session,
-                                                     r_instance['name'])
+        rescue_vm_ref = vm_util.get_vm_ref_from_name(self._session,
+                                                     instance_name)
         self._volumeops.attach_disk_to_vm(
                                 rescue_vm_ref, r_instance,
                                 adapter_type, disk_type, vmdk_path,
@@ -1077,9 +1076,17 @@
          unit_number) = vm_util.get_vmdk_path_and_adapter_type(
                 hardware_devices, uuid=instance['uuid'])
         r_instance = copy.deepcopy(instance)
+        instance_name = r_instance['uuid'] + self._rescue_suffix
         r_instance['name'] = r_instance['name'] + self._rescue_suffix
-        r_instance['uuid'] = r_instance['uuid'] + self._rescue_suffix
-        self.destroy(r_instance, None)
+        # detach the original instance disk from the rescue disk
+        vm_rescue_ref = vm_util.get_vm_ref_from_name(self._session,
+                                                     instance_name)
+        hardware_devices = self._session._call_method(vim_util,
+                        "get_dynamic_property", vm_rescue_ref,
+                        "VirtualMachine", "config.hardware.device")
+        device = vm_util.get_vmdk_volume_disk(hardware_devices, path=vmdk_path)
+        self._volumeops.detach_disk_from_vm(vm_rescue_ref, r_instance, device)
+        self.destroy(r_instance, None, instance_name=instance_name)
         self._power_on(instance)
 
     def power_off(self, instance):
@@ -1229,9 +1236,6 @@
             LOG.warn(_("In vmwareapi:vmops:confirm_migration, got this "
                      "exception while destroying the VM: %s") % str(excep))
 
-        if network_info:
-            self.unplug_vifs(instance, network_info)
-
     def finish_revert_migration(self, instance, network_info,
                                 block_device_info, power_on=True):
         """Finish reverting a resize."""
@@ -1596,14 +1600,6 @@
         client_factory = self._session._get_vim().client.factory
         self._set_machine_id(client_factory, instance, network_info)
 
-    def plug_vifs(self, instance, network_info):
-        """Plug VIFs into networks."""
-        pass
-
-    def unplug_vifs(self, instance, network_info):
-        """Unplug VIFs from networks."""
-        pass
-
 
 class VMwareVCVMOps(VMwareVMOps):
     """Management class for VM-related tasks.
diff -ru nova/virt/vmwareapi/vm_util.py nova.new/virt/vmwareapi/vm_util.py
--- nova/virt/vmwareapi/vm_util.py	2014-04-03 14:49:46.000000000 -0400
+++ nova.new/virt/vmwareapi/vm_util.py	2014-04-19 13:44:42.510933664 -0400
@@ -57,11 +57,11 @@
     return datastore_url, path.strip()
 
 
-def get_vm_create_spec(client_factory, instance, data_store_name,
+def get_vm_create_spec(client_factory, instance, name, data_store_name,
                        vif_infos, os_type="otherGuest"):
     """Builds the VM Create spec."""
     config_spec = client_factory.create('ns0:VirtualMachineConfigSpec')
-    config_spec.name = instance['uuid']
+    config_spec.name = name
     config_spec.guestId = os_type
 
     # Allow nested ESX instances to host 64 bit VMs.
@@ -1043,13 +1043,14 @@
             return device
 
 
-def get_vmdk_volume_disk(hardware_devices):
+def get_vmdk_volume_disk(hardware_devices, path=None):
     if hardware_devices.__class__.__name__ == "ArrayOfVirtualDevice":
         hardware_devices = hardware_devices.VirtualDevice
 
     for device in hardware_devices:
         if (device.__class__.__name__ == "VirtualDisk"):
-            return device
+            if not path or path == device.backing.fileName:
+                return device
 
 
 def get_res_pool_ref(session, cluster, node_mo_id):

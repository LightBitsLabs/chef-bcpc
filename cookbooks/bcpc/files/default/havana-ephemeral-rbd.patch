diff --git a/nova/compute/manager.py b/nova/compute/manager.py
index 4e70c94..0280287 100644
--- a/nova/compute/manager.py
+++ b/nova/compute/manager.py
@@ -4145,11 +4145,18 @@ class ComputeManager(manager.SchedulerDependentManager):
         # No instance booting at source host, but instance dir
         # must be deleted for preparing next block migration
         # must be deleted for preparing next live migration w/o shared storage
-        is_shared_storage = True
+        is_shared_block_storage = True
+        is_shared_instance_path = True
         if migrate_data:
-            is_shared_storage = migrate_data.get('is_shared_storage', True)
-        if block_migration or not is_shared_storage:
-            self.driver.destroy(instance_ref, network_info)
+            is_shared_block_storage = migrate_data.get(
+                    'is_shared_block_storage', True)
+            is_shared_instance_path = migrate_data.get(
+                    'is_shared_instance_path', True)
+
+        if block_migration or not is_shared_instance_path:
+            self.driver.destroy(instance_ref, network_info,
+                    destroy_disks=not is_shared_block_storage,
+                    migrate_data=migrate_data)
         else:
             # self.driver.destroy() usually performs  vif unplugging
             # but we must do it explicitly here when block_migration
@@ -4272,20 +4279,24 @@ class ComputeManager(manager.SchedulerDependentManager):
         # Also Volume backed live migration w/o shared storage needs to delete
         # newly created instance-xxx dir on the destination as a part of its
         # rollback process
-        is_volume_backed = False
-        is_shared_storage = True
+        is_shared_block_storage = False
+        is_shared_instance_path = False
         if migrate_data:
-            is_volume_backed = migrate_data.get('is_volume_backed', False)
-            is_shared_storage = migrate_data.get('is_shared_storage', True)
-        if block_migration or (is_volume_backed and not is_shared_storage):
+            is_shared_block_storage = migrate_data.get(
+                    'is_shared_block_storage', True)
+            is_shared_instance_path = migrate_data.get(
+                    'is_shared_instance_path', True)
+        if block_migration or (
+                is_shared_block_storage and not is_shared_instance_path):
             self.compute_rpcapi.rollback_live_migration_at_destination(context,
-                    instance, dest)
+                    instance, dest, destroy_disks=not is_shared_block_storage)
 
         self._notify_about_instance_usage(context, instance,
                                           "live_migration._rollback.end")
 
     @wrap_exception()
-    def rollback_live_migration_at_destination(self, context, instance):
+    def rollback_live_migration_at_destination(self, context, instance,
+                                               destroy_disks=True):
         """Cleaning up image directory that is created pre_live_migration.
 
         :param context: security context
@@ -4304,7 +4315,8 @@ class ComputeManager(manager.SchedulerDependentManager):
         #             from remote volumes if necessary
         block_device_info = self._get_instance_volume_block_device_info(
                             context, instance)
-        self.driver.destroy(instance, network_info, block_device_info)
+        self.driver.destroy(instance, network_info, block_device_info,
+                            destroy_disks)
         self._notify_about_instance_usage(
                         context, instance, "live_migration.rollback.dest.end",
                         network_info=network_info)
diff --git a/nova/compute/rpcapi.py b/nova/compute/rpcapi.py
index f115857..04af3d4 100644
--- a/nova/compute/rpcapi.py
+++ b/nova/compute/rpcapi.py
@@ -678,12 +678,13 @@ class ComputeAPI(rpcclient.RpcProxy):
                    instance=instance, migration=migration,
                    reservations=reservations)
 
-    def rollback_live_migration_at_destination(self, ctxt, instance, host):
+    def rollback_live_migration_at_destination(self, ctxt, instance, host,
+                                               destroy_disks=True):
         instance_p = jsonutils.to_primitive(instance)
         cctxt = self.client.prepare(server=host,
                 version=_get_version('2.0'))
         cctxt.cast(ctxt, 'rollback_live_migration_at_destination',
-                   instance=instance_p)
+                   instance=instance_p, destroy_disks=destroy_disks)
 
     def run_instance(self, ctxt, instance, host, request_spec,
                      filter_properties, requested_networks,
diff --git a/nova/image/glance.py b/nova/image/glance.py
index cc64da7..5790c6a 100644
--- a/nova/image/glance.py
+++ b/nova/image/glance.py
@@ -295,7 +295,7 @@ class GlanceImageService(object):
         base_image_meta = self._translate_from_glance(image)
         return base_image_meta
 
-    def _get_locations(self, context, image_id):
+    def get_locations(self, context, image_id):
         """Returns the direct url representing the backend storage location,
         or None if this attribute is not shown by Glance.
         """
@@ -327,7 +327,7 @@ class GlanceImageService(object):
     def download(self, context, image_id, data=None, dst_path=None):
         """Calls out to Glance for data and writes data."""
         if CONF.allowed_direct_url_schemes and dst_path is not None:
-            locations = self._get_locations(context, image_id)
+            locations = self.get_locations(context, image_id)
             for entry in locations:
                 loc_url = entry['url']
                 loc_meta = entry['metadata']
diff --git a/nova/virt/driver.py b/nova/virt/driver.py
index 973b903..f9c99bb 100644
--- a/nova/virt/driver.py
+++ b/nova/virt/driver.py
@@ -242,7 +242,7 @@ class ComputeDriver(object):
         raise NotImplementedError()
 
     def destroy(self, instance, network_info, block_device_info=None,
-                destroy_disks=True):
+                destroy_disks=True, migrate_data=None):
         """Destroy (shutdown and delete) the specified instance.
 
         If the instance is not found (for example if networking failed), this
diff --git a/nova/virt/fake.py b/nova/virt/fake.py
index 41825a9..6af4e62 100644
--- a/nova/virt/fake.py
+++ b/nova/virt/fake.py
@@ -207,7 +207,7 @@ class FakeDriver(driver.ComputeDriver):
         pass
 
     def destroy(self, instance, network_info, block_device_info=None,
-                destroy_disks=True, context=None):
+                destroy_disks=True, context=None, migrate_data=None):
         key = instance['name']
         if key in self.instances:
             del self.instances[key]
diff --git a/nova/virt/images.py b/nova/virt/images.py
index 6d20e65..e667c15 100644
--- a/nova/virt/images.py
+++ b/nova/virt/images.py
@@ -179,6 +179,26 @@ def convert_image(source, dest, out_format, run_as_root=False):
     utils.execute(*cmd, run_as_root=run_as_root)
 
 
+def direct_fetch(context, image_href, backend_dest, _user_id, _project_id,
+                 max_size=0):
+    """Allow an image backend to fetch directly from the glance backend.
+
+    :backend_dest: the image backend, which must have a direct_fetch
+                   method accepting a list of image locations. This method
+                   should raise exceptions.ImageUnacceptable if the image
+                   cannot be downloaded directly.
+    """
+    # TODO(jdurgin): improve auth handling as noted in fetch()
+    image_service, image_id = glance.get_remote_image_service(context,
+                                                              image_href)
+    locations = image_service.get_locations(context, image_id)
+    image_meta = image_service.show(context, image_id)
+
+    LOG.debug(_('Image locations are: %(locs)s') % {'locs': locations})
+    backend_dest.direct_fetch(image_id, image_meta, locations,
+                              max_size=max_size)
+
+
 def fetch(context, image_href, path, _user_id, _project_id, max_size=0):
     # TODO(vish): Improve context handling and add owner and auth data
     #             when it is added to glance.  Right now there is no
diff --git a/nova/virt/libvirt/driver.py b/nova/virt/libvirt/driver.py
index 34f4f42..6cd065d 100644
--- a/nova/virt/libvirt/driver.py
+++ b/nova/virt/libvirt/driver.py
@@ -829,10 +829,11 @@ class LibvirtDriver(driver.ComputeDriver):
             self._destroy(instance)
 
     def destroy(self, instance, network_info, block_device_info=None,
-                destroy_disks=True, context=None):
+                destroy_disks=True, context=None, migrate_data=None):
         self._destroy(instance)
         self._cleanup(instance, network_info, block_device_info,
-                      destroy_disks, context=context)
+                      destroy_disks, context=context,
+                      migrate_data=migrate_data)
 
     def _undefine_domain(self, instance):
         try:
@@ -866,7 +867,7 @@ class LibvirtDriver(driver.ComputeDriver):
                               {'errcode': errcode, 'e': e}, instance=instance)
 
     def _cleanup(self, instance, network_info, block_device_info,
-                 destroy_disks, context=None):
+                 destroy_disks, context=None, migrate_data=None):
         self._undefine_domain(instance)
         self.unplug_vifs(instance, network_info)
         retry = True
@@ -941,9 +942,12 @@ class LibvirtDriver(driver.ComputeDriver):
                                  {'vol_id': vol.get('volume_id'), 'exc': exc},
                                  instance=instance)
 
-        if destroy_disks:
+        if destroy_disks or (
+                migrate_data and migrate_data.get('is_shared_block_storage',
+                                                  False)):
             self._delete_instance_files(instance)
 
+        if destroy_disks:
             self._cleanup_lvm(instance)
             #NOTE(haomai): destory volumes if needed
             if CONF.libvirt_images_type == 'rbd':
@@ -952,7 +956,7 @@ class LibvirtDriver(driver.ComputeDriver):
     def _cleanup_rbd(self, instance):
         pool = CONF.libvirt_images_rbd_pool
         volumes = libvirt_utils.list_rbd_volumes(pool)
-        pattern = instance['name']
+        pattern = instance['uuid']
 
         def belongs_to_instance(disk):
             return disk.startswith(pattern)
@@ -2380,13 +2384,15 @@ class LibvirtDriver(driver.ComputeDriver):
             if size == 0 or suffix == '.rescue':
                 size = None
 
-            image('disk').cache(fetch_func=libvirt_utils.fetch_image,
-                                context=context,
-                                filename=root_fname,
-                                size=size,
-                                image_id=disk_images['image_id'],
-                                user_id=instance['user_id'],
-                                project_id=instance['project_id'])
+            disk_image = image('disk')
+            disk_image.cache(fetch_func=libvirt_utils.fetch_image,
+                             context=context,
+                             filename=root_fname,
+                             size=size,
+                             backend_dest=disk_image,
+                             image_id=disk_images['image_id'],
+                             user_id=instance['user_id'],
+                             project_id=instance['project_id'])
 
         # Lookup the filesystem type if required
         os_type_with_default = disk.get_fs_type_for_os_type(
@@ -3855,16 +3861,15 @@ class LibvirtDriver(driver.ComputeDriver):
         # Checking shared storage connectivity
         # if block migration, instances_paths should not be on shared storage.
         source = CONF.host
-        filename = dest_check_data["filename"]
-        block_migration = dest_check_data["block_migration"]
-        is_volume_backed = dest_check_data.get('is_volume_backed', False)
-        has_local_disks = bool(
-                jsonutils.loads(self.get_instance_disk_info(instance['name'])))
 
-        shared = self._check_shared_storage_test_file(filename)
+        dest_check_data.update({'is_shared_block_storage':
+                self._is_shared_block_storage(instance, dest_check_data)})
+        dest_check_data.update({'is_shared_instance_path':
+                self._is_shared_instance_path(dest_check_data)})
 
-        if block_migration:
-            if shared:
+        if dest_check_data['block_migration']:
+            if (dest_check_data['is_shared_block_storage'] or
+                    dest_check_data['is_shared_instance_path']):
                 reason = _("Block migration can not be used "
                            "with shared storage.")
                 raise exception.InvalidLocalStorage(reason=reason, path=source)
@@ -3872,11 +3877,11 @@ class LibvirtDriver(driver.ComputeDriver):
                                     dest_check_data['disk_available_mb'],
                                     dest_check_data['disk_over_commit'])
 
-        elif not shared and (not is_volume_backed or has_local_disks):
+        elif not (dest_check_data['is_shared_block_storage'] or
+                  dest_check_data['is_shared_instance_path']):
             reason = _("Live migration can not be used "
                        "without shared storage.")
             raise exception.InvalidSharedStorage(reason=reason, path=source)
-        dest_check_data.update({"is_shared_storage": shared})
 
         # NOTE(mikal): include the instance directory name here because it
         # doesn't yet exist on the destination but we want to force that
@@ -3887,6 +3892,21 @@ class LibvirtDriver(driver.ComputeDriver):
 
         return dest_check_data
 
+    def _is_shared_block_storage(self, instance, dest_check_data):
+        remote_image_backends = ('rbd',)
+        is_local_image = CONF.libvirt_images_type not in remote_image_backends
+
+        is_volume_backed = dest_check_data.get('is_volume_backed', False)
+        has_local_disks = bool(
+                jsonutils.loads(self.get_instance_disk_info(instance['name'])))
+
+        return (not is_local_image or (
+                    is_volume_backed and not has_local_disks))
+
+    def _is_shared_instance_path(self, dest_check_data):
+        return self._check_shared_storage_test_file(
+                    dest_check_data["filename"])
+
     def _assert_dest_node_has_enough_disk(self, context, instance,
                                              available_mb, disk_over_commit):
         """Checks if destination has enough disk for block migration."""
@@ -4135,17 +4155,21 @@ class LibvirtDriver(driver.ComputeDriver):
                            network_info, disk_info, migrate_data=None):
         """Preparation live migration."""
         # Steps for volume backed instance live migration w/o shared storage.
-        is_shared_storage = True
+        is_shared_block_storage = True
+        is_shared_instance_path = True
         is_volume_backed = False
         is_block_migration = True
         instance_relative_path = None
         if migrate_data:
-            is_shared_storage = migrate_data.get('is_shared_storage', True)
+            is_shared_block_storage = migrate_data.get(
+                    'is_shared_block_storage', True)
+            is_shared_instance_path = migrate_data.get(
+                    'is_shared_instance_path', True)
             is_volume_backed = migrate_data.get('is_volume_backed', False)
             is_block_migration = migrate_data.get('block_migration', True)
             instance_relative_path = migrate_data.get('instance_relative_path')
 
-        if not is_shared_storage:
+        if not is_shared_instance_path:
             # NOTE(mikal): this doesn't use libvirt_utils.get_instance_path
             # because we are ensuring that the same instance directory name
             # is used as was at the source
@@ -4159,11 +4183,12 @@ class LibvirtDriver(driver.ComputeDriver):
                 raise exception.DestinationDiskExists(path=instance_dir)
             os.mkdir(instance_dir)
 
+        if not is_shared_block_storage:
             # Ensure images and backing files are present.
             self._create_images_and_backing(context, instance, instance_dir,
                                             disk_info)
 
-        if is_volume_backed and not (is_block_migration or is_shared_storage):
+        if not (is_block_migration or is_shared_instance_path):
             # Touch the console.log file, required by libvirt.
             console_file = self._get_console_log_path(instance)
             libvirt_utils.file_open(console_file, 'a').close()
diff --git a/nova/virt/libvirt/imagebackend.py b/nova/virt/libvirt/imagebackend.py
index 51872cf..4d6238c 100644
--- a/nova/virt/libvirt/imagebackend.py
+++ b/nova/virt/libvirt/imagebackend.py
@@ -18,6 +18,7 @@
 import abc
 import contextlib
 import os
+import urllib
 
 from oslo.config import cfg
 
@@ -35,8 +36,10 @@ from nova.virt.libvirt import utils as libvirt_utils
 
 
 try:
+    import rados
     import rbd
 except ImportError:
+    rados = None
     rbd = None
 
 
@@ -69,6 +72,8 @@ CONF = cfg.CONF
 CONF.register_opts(__imagebackend_opts)
 CONF.import_opt('base_dir_name', 'nova.virt.libvirt.imagecache')
 CONF.import_opt('preallocate_images', 'nova.virt.driver')
+CONF.import_opt('rbd_user', 'nova.virt.libvirt.volume')
+CONF.import_opt('rbd_secret_uuid', 'nova.virt.libvirt.volume')
 
 LOG = logging.getLogger(__name__)
 
@@ -193,8 +198,7 @@ class Image(object):
                           (CONF.preallocate_images, self.path))
         return can_fallocate
 
-    @staticmethod
-    def verify_base_size(base, size, base_size=0):
+    def verify_base_size(self, base, size, base_size=0):
         """Check that the base image is not larger than size.
            Since images can't be generally shrunk, enforce this
            constraint taking account of virtual image size.
@@ -213,7 +217,7 @@ class Image(object):
             return
 
         if size and not base_size:
-            base_size = disk.get_disk_size(base)
+            base_size = self.get_disk_size(base)
 
         if size < base_size:
             msg = _('%(base)s virtual size %(base_size)s '
@@ -223,6 +227,9 @@ class Image(object):
                               'size': size})
             raise exception.InstanceTypeDiskTooSmall()
 
+    def get_disk_size(self, name):
+        disk.get_disk_size(name)
+
     def snapshot_create(self):
         raise NotImplementedError()
 
@@ -232,6 +239,14 @@ class Image(object):
     def snapshot_delete(self):
         raise NotImplementedError()
 
+    def direct_fetch(self, image_id, image_meta, image_locations, max_size=0):
+        """Create an image from a direct image location.
+
+        :raises: exception.ImageUnacceptable if it cannot be fetched directly
+        """
+        reason = _('direct_fetch() is not implemented')
+        raise exception.ImageUnacceptable(image_id=image_id, reason=reason)
+
 
 class Raw(Image):
     def __init__(self, instance=None, disk_name=None, path=None,
@@ -435,32 +450,110 @@ class Lvm(Image):
         libvirt_utils.execute(*cmd, run_as_root=True, attempts=3)
 
 
+class RBDVolumeProxy(object):
+    """Context manager for dealing with an existing rbd volume.
+
+    This handles connecting to rados and opening an ioctx automatically, and
+    otherwise acts like a librbd Image object.
+
+    The underlying librados client and ioctx can be accessed as the attributes
+    'client' and 'ioctx'.
+    """
+    def __init__(self, driver, name, pool=None, snapshot=None,
+                 read_only=False):
+        client, ioctx = driver._connect_to_rados(pool)
+        try:
+            self.volume = driver.rbd.Image(ioctx, str(name),
+                                           snapshot=libvirt_utils.ascii_str(
+                                                        snapshot),
+                                           read_only=read_only)
+        except driver.rbd.Error:
+            LOG.exception(_("error opening rbd image %s"), name)
+            driver._disconnect_from_rados(client, ioctx)
+            raise
+        self.driver = driver
+        self.client = client
+        self.ioctx = ioctx
+
+    def __enter__(self):
+        return self
+
+    def __exit__(self, type_, value, traceback):
+        try:
+            self.volume.close()
+        finally:
+            self.driver._disconnect_from_rados(self.client, self.ioctx)
+
+    def __getattr__(self, attrib):
+        return getattr(self.volume, attrib)
+
+
+class RADOSClient(object):
+    """Context manager to simplify error handling for connecting to ceph."""
+    def __init__(self, driver, pool=None):
+        self.driver = driver
+        self.cluster, self.ioctx = driver._connect_to_rados(pool)
+
+    def __enter__(self):
+        return self
+
+    def __exit__(self, type_, value, traceback):
+        self.driver._disconnect_from_rados(self.cluster, self.ioctx)
+
+
 class Rbd(Image):
     def __init__(self, instance=None, disk_name=None, path=None,
                  snapshot_name=None, **kwargs):
         super(Rbd, self).__init__("block", "rbd", is_block_dev=True)
         if path:
             try:
-                self.rbd_name = path.split('/')[1]
+                self.rbd_name = str(path.split('/')[1])
             except IndexError:
                 raise exception.InvalidDevicePath(path=path)
         else:
-            self.rbd_name = '%s_%s' % (instance['name'], disk_name)
+            self.rbd_name = str('%s_%s' % (instance['uuid'], disk_name))
         self.snapshot_name = snapshot_name
         if not CONF.libvirt_images_rbd_pool:
             raise RuntimeError(_('You should specify'
                                  ' libvirt_images_rbd_pool'
                                  ' flag to use rbd images.'))
-        self.pool = CONF.libvirt_images_rbd_pool
-        self.ceph_conf = CONF.libvirt_images_rbd_ceph_conf
+        self.pool = str(CONF.libvirt_images_rbd_pool)
+        self.ceph_conf = libvirt_utils.ascii_str(
+                         CONF.libvirt_images_rbd_ceph_conf)
+        self.rbd_user = libvirt_utils.ascii_str(CONF.rbd_user)
         self.rbd = kwargs.get('rbd', rbd)
+        self.rados = kwargs.get('rados', rados)
+
+        self.path = 'rbd:%s/%s' % (self.pool, self.rbd_name)
+        if self.rbd_user:
+            self.path += ':id=' + self.rbd_user
+        if self.ceph_conf:
+            self.path += ':conf=' + self.ceph_conf
+
+    def _connect_to_rados(self, pool=None):
+        client = self.rados.Rados(rados_id=self.rbd_user,
+                                  conffile=self.ceph_conf)
+        try:
+            client.connect()
+            pool_to_open = str(pool or self.pool)
+            ioctx = client.open_ioctx(pool_to_open)
+            return client, ioctx
+        except self.rados.Error:
+            # shutdown cannot raise an exception
+            client.shutdown()
+            raise
+
+    def _disconnect_from_rados(self, client, ioctx):
+        # closing an ioctx cannot raise an exception
+        ioctx.close()
+        client.shutdown()
 
     def _supports_layering(self):
         return hasattr(self.rbd, 'RBD_FEATURE_LAYERING')
 
     def _ceph_args(self):
         args = []
-        args.extend(['--id', CONF.rbd_user])
+        args.extend(['--id', self.rbd_user])
         args.extend(['--conf', self.ceph_conf])
         return args
 
@@ -526,6 +619,17 @@ class Rbd(Image):
 
         return False
 
+    def _resize(self, size):
+        with RBDVolumeProxy(self, self.rbd_name) as vol:
+            vol.resize(int(size))
+
+    def _size(self):
+        return self.get_disk_size(self.rbd_name)
+
+    def get_disk_size(self, name):
+        with RBDVolumeProxy(self, name) as vol:
+            return vol.size()
+
     def create_image(self, prepare_template, base, size, *args, **kwargs):
         if self.rbd is None:
             raise RuntimeError(_('rbd python libraries not found'))
@@ -536,29 +640,104 @@ class Rbd(Image):
             old_format = False
             features = self.rbd.RBD_FEATURE_LAYERING
 
-        if not os.path.exists(base):
+        if not self.check_image_exists():
             prepare_template(target=base, max_size=size, *args, **kwargs)
         else:
             self.verify_base_size(base, size)
 
-        # keep using the command line import instead of librbd since it
-        # detects zeroes to preserve sparseness in the image
-        args = ['--pool', self.pool, base, self.rbd_name]
-        if self._supports_layering():
-            args += ['--new-format']
-        args += self._ceph_args()
-        libvirt_utils.import_rbd_image(*args)
+        # prepare_template may have created the image via direct_fetch()
+        if not self.check_image_exists():
+            # keep using the command line import instead of librbd since it
+            # detects zeroes to preserve sparseness in the image
+            args = ['--pool', self.pool, base, self.rbd_name]
+            if self._supports_layering():
+                args += ['--new-format']
+            args += self._ceph_args()
+            libvirt_utils.import_rbd_image(*args)
+
+        if size and self._size() < size:
+            self._resize(size)
 
     def snapshot_create(self):
         pass
 
     def snapshot_extract(self, target, out_format):
-        snap = 'rbd:%s/%s' % (self.pool, self.rbd_name)
-        images.convert_image(snap, target, out_format)
+        images.convert_image(self.path, target, out_format)
 
     def snapshot_delete(self):
         pass
 
+    def _parse_location(self, location):
+        prefix = 'rbd://'
+        if not location.startswith(prefix):
+            reason = _('Not stored in rbd')
+            raise exception.ImageUnacceptable(image_id=location, reason=reason)
+        pieces = map(urllib.unquote, location[len(prefix):].split('/'))
+        if any(map(lambda p: p == '', pieces)):
+            reason = _('Blank components')
+            raise exception.ImageUnacceptable(image_id=location, reason=reason)
+        if len(pieces) != 4:
+            reason = _('Not an rbd snapshot')
+            raise exception.ImageUnacceptable(image_id=location, reason=reason)
+        return pieces
+
+    def _get_fsid(self):
+        with RADOSClient(self) as client:
+            return client.cluster.get_fsid()
+
+    def _is_cloneable(self, image_location):
+        try:
+            fsid, pool, image, snapshot = self._parse_location(image_location)
+        except exception.ImageUnacceptable as e:
+            LOG.debug(_('not cloneable: %s'), e)
+            return False
+
+        if self._get_fsid() != fsid:
+            reason = _('%s is in a different ceph cluster') % image_location
+            LOG.debug(reason)
+            return False
+
+        # check that we can read the image
+        try:
+            with RBDVolumeProxy(self, image,
+                                pool=pool,
+                                snapshot=snapshot,
+                                read_only=True):
+                return True
+        except self.rbd.Error as e:
+            LOG.debug(_('Unable to open image %(loc)s: %(err)s') %
+                      dict(loc=image_location, err=e))
+            return False
+
+    def _clone(self, pool, image, snapshot):
+        with RADOSClient(self, str(pool)) as src_client:
+            with RADOSClient(self) as dest_client:
+                self.rbd.RBD().clone(src_client.ioctx,
+                                     str(image),
+                                     str(snapshot),
+                                     dest_client.ioctx,
+                                     self.rbd_name,
+                                     features=self.rbd.RBD_FEATURE_LAYERING)
+
+    def direct_fetch(self, image_id, image_meta, image_locations, max_size=0):
+        if self.check_image_exists():
+            return
+        if image_meta.get('disk_format') != 'raw':
+            reason = _('Image is not raw format')
+            raise exception.ImageUnacceptable(image_id=image_id, reason=reason)
+        if not self._supports_layering():
+            reason = _('installed version of librbd does not support cloning')
+            raise exception.ImageUnacceptable(image_id=image_id, reason=reason)
+
+        for location in image_locations:
+            url = location['url']
+            if self._is_cloneable(url):
+                prefix, pool, image, snapshot = self._parse_location(url)
+                return self._clone(pool, image, snapshot)
+
+        reason = _('No image locations are accessible')
+        raise exception.ImageUnacceptable(image_id=image_id, reason=reason)
+
 
 class Backend(object):
     def __init__(self, use_cow):
diff --git a/nova/virt/libvirt/utils.py b/nova/virt/libvirt/utils.py
index d7c92b7..a670608 100644
--- a/nova/virt/libvirt/utils.py
+++ b/nova/virt/libvirt/utils.py
@@ -45,6 +45,13 @@ CONF.register_opts(libvirt_opts)
 CONF.import_opt('instances_path', 'nova.compute.manager')
 LOG = logging.getLogger(__name__)
 
+try:
+    import rados
+    import rbd
+except ImportError:
+    rados = None
+    rbd = None
+
 
 def execute(*args, **kwargs):
     return utils.execute(*args, **kwargs)
@@ -258,6 +265,33 @@ def create_lvm_image(vg, lv, size, sparse=False):
     execute(*cmd, run_as_root=True, attempts=3)
 
 
+def ascii_str(s):
+    """Convert a string to ascii, or return None if the input is None.
+
+    This is useful when a parameter is None by default, or a string. LibRBD
+    only accepts ascii, hence the need for conversion.
+    """
+    if s is None:
+        return s
+    return str(s)
+
+
+def _connect_to_rados(pool=None):
+    ceph_conf = ascii_str(CONF.libvirt_images_rbd_ceph_conf)
+    rbd_user = ascii_str(CONF.rbd_user)
+    client = rados.Rados(rados_id=rbd_user,
+                              conffile=ceph_conf)
+    try:
+        client.connect()
+        pool_to_open = str(pool)
+        ioctx = client.open_ioctx(pool_to_open)
+        return client, ioctx
+    except rados.Error:
+        # shutdown cannot raise an exception
+        client.shutdown()
+        raise
+
+
 def import_rbd_image(*args):
     execute('rbd', 'import', *args)
 
@@ -267,18 +301,20 @@ def list_rbd_volumes(pool):
 
     :param pool: ceph pool name
     """
-    out, err = utils.execute('rbd', '-p', pool, 'ls')
-
-    return [line.strip() for line in out.splitlines()]
+    client, ioctx = _connect_to_rados(pool)
+    rbd_inst = rbd.RBD()
+    return rbd_inst.list(ioctx)
 
 
 def remove_rbd_volumes(pool, *names):
     """Remove one or more rbd volume."""
+    client, ioctx = _connect_to_rados(pool)
+    rbd_inst = rbd.RBD()
     for name in names:
-        rbd_remove = ('rbd', '-p', pool, 'rm', name)
         try:
-            execute(*rbd_remove, attempts=3, run_as_root=True)
-        except processutils.ProcessExecutionError:
+            # Retry if ImageBusy raised
+            rbd_inst.remove(ioctx, name)
+        except rbd.ImageNotFound, rbd.ImageHasSnapshots:
             LOG.warn(_("rbd remove %(name)s in pool %(pool)s failed"),
                      {'name': name, 'pool': pool})
 
@@ -639,8 +675,16 @@ def get_fs_info(path):
             'used': used}
 
 
-def fetch_image(context, target, image_id, user_id, project_id, max_size=0):
+def fetch_image(context, target, image_id, user_id, project_id, max_size=0,
+                backend_dest=None):
     """Grab image."""
+    if backend_dest:
+        try:
+            images.direct_fetch(context, image_id, backend_dest,
+                                user_id, project_id)
+            return
+        except exception.ImageUnacceptable:
+            LOG.debug(_('could not fetch directly, falling back to download'))
     images.fetch_to_raw(context, image_id, target, user_id, project_id,
                         max_size=max_size)
 
